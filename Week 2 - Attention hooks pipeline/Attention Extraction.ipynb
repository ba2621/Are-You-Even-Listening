{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformer_lens import HookedTransformer\n",
        "\n",
        "# HF model name\n",
        "HF_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Device (GPU preferred)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Path to your tokenized dataset file\n",
        "TOKENIZED_PATH = \"/content/token_segmentation_metadata.json\"\n"
      ],
      "metadata": {
        "id": "y4E9VAFj4o99"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ],
      "metadata": {
        "id": "se6cXQjA48Au",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "858a07b040a54909b8f601abbc548d82",
            "b24b241020b247528322700f9610b648",
            "c35586858782404f866948a7393ee9ba",
            "8fcc609d8bfc4efcb205637a6d937ec6",
            "664809668490464d8157d176a9fc905b",
            "5e9afe5342a04643a03eed6630300acc",
            "d98123303a654fa8a22c8196403674dd",
            "42960177390b48e6a4d01bff5d2860a9",
            "f185a732d9a647a5b70afd349d0c6c6a",
            "ad38c1f582cb40bda37be292c7f5a09f",
            "edeaf1d8e37d406f9726d98407712e6f",
            "dde084764c4e424a8d8dfa4383f4a7c1",
            "b628b5a3b9cf4e0c9c343235227291dc",
            "3e7751039060425395fbe46d173b26b0",
            "9d2c0df19e6c4a11b31e6e326a565908",
            "f3cd64a302404b6fad352491e8159a88",
            "b8871ff7e13d4a9894cc74090bbaf6c6",
            "6ec607bbcafa47fd9438bfef6fb217ee",
            "c5e48d6098fd4d2c855303d879044a6d",
            "8477fe2d711643319e7038c40b32b154",
            "81621ea05741480f9da53bfae0c8d716",
            "8a7f6493831443ffbd5616c0b523253d",
            "a25621b27b8b4368b1f35a47eddf3f32",
            "2c90fb3045c54b19b98178b9d549332c",
            "5ab3b0d8d1444998be82061695408880",
            "581acf062756404c98aebfbf9c7bfb8f",
            "d176165ef905497f827dafb8d2200dc6",
            "3ea336e2fbac45bfaf5c38ad3258bf95",
            "25d8f941ad9d4e5e84ccfea08b5fae78"
          ]
        },
        "collapsed": true,
        "outputId": "dbc949c0-bd24-4097-ea50-1ab0db7d0764"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "858a07b040a54909b8f601abbc548d82"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokenizer(model_name: str = HF_MODEL_NAME):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    # Ensure pad token exists\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = load_tokenizer()\n",
        "print(\"Tokenizer loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4CTg78x3cFC",
        "outputId": "95922f11-0226-469e-ed9e-651079a6709f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()  # Clear GPU memory cache\n"
      ],
      "metadata": {
        "id": "PWCcQkGF31CY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name: str = HF_MODEL_NAME, device: str = DEVICE):\n",
        "    model = HookedTransformer.from_pretrained(\n",
        "        model_name,\n",
        "        device=device,\n",
        "        dtype=torch.float16,  # Use FP16 for reduced memory usage\n",
        "    )\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "print(\"Model loaded on:\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "70205dc4efe841d495e1b6778075b50c",
            "dceb40fbea13489fbeebb555d8946659",
            "4c17c991f65d436e8183dd778cd2f1f4",
            "1ad42372ad2e4cdbb9f561b5a9aa522c",
            "dffaaf9f49d449ea87c38128c87f4fdd",
            "a0990eee9a1e447bbed771078ddfd602",
            "e83743f167324db3925f09d5adf23847",
            "c26366d099b44c4f9edc041e554e0857",
            "4ef1a2b31eb543f890e093156c79be19",
            "10f7fb3d60ba4b649d0f64791dd4d6d9",
            "db0c9aa00e3c468eaeb37c3411a5f74d"
          ]
        },
        "id": "oCofBlVn3d7z",
        "outputId": "3a9cf785-49ef-43e5-ac98-39b24e65263c",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70205dc4efe841d495e1b6778075b50c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
            "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n",
            "Model loaded on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_attention_store():\n",
        "    \"\"\"\n",
        "    Creates an empty Python dictionary that will be filled\n",
        "    during the forward pass with attention matrices.\n",
        "    \"\"\"\n",
        "    return {}\n",
        "\n",
        "def save_attention_hook(attn, hook, store):\n",
        "    \"\"\"\n",
        "    Hook called by TransformerLens every time attention is computed\n",
        "    in a specific layer.\n",
        "    \"\"\"\n",
        "    layer_idx = hook.layer()\n",
        "    store[layer_idx] = attn.detach().cpu()\n",
        "\n",
        "print(\"Attention hook system ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA3DvBLc3f-k",
        "outputId": "c7c2387e-68aa-4b5a-d4e6-b1432f9efce9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention hook system ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_hooks(model, store):\n",
        "    \"\"\"\n",
        "    Attach hooks to the attention *pattern* (softmax probs) in every layer.\n",
        "    \"\"\"\n",
        "    hooks = []\n",
        "    all_hooks = set(hp.name for hp in model.hook_points())\n",
        "\n",
        "    for layer in range(model.cfg.n_layers):\n",
        "        hook_name = f\"blocks.{layer}.attn.hook_pattern\"\n",
        "\n",
        "        if hook_name not in all_hooks:\n",
        "            raise KeyError(\n",
        "                f\"{hook_name} not found in model.hook_points(). \"\n",
        "                f\"Available attention hooks include: \"\n",
        "                f\"{[h for h in all_hooks if 'attn' in h]}\"\n",
        "            )\n",
        "\n",
        "        hook_fn = partial(save_attention_hook, store=store)\n",
        "        hooks.append((hook_name, hook_fn))\n",
        "\n",
        "    return hooks\n"
      ],
      "metadata": {
        "id": "gx4nhTLo4T_l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_and_get_attention(model, row):\n",
        "    \"\"\"\n",
        "    Core function to extract attention weights.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : HookedTransformer\n",
        "        The TransformerLens model loaded earlier.\n",
        "\n",
        "    row : dict\n",
        "        A dictionary containing tokenized information, including 'input_ids'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    final_store : dict\n",
        "        Mapping layer_idx -> attention tensor of shape [n_heads, L, L],\n",
        "        all moved to CPU for easy downstream processing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the input_ids from the row (which contains the concatenated tokens)\n",
        "    input_ids = row['input_ids']\n",
        "\n",
        "    # Ensure tokens = [1, L]\n",
        "    tokens = torch.tensor([input_ids], device=DEVICE, dtype=torch.long)  # Create a tensor with shape [1, L]\n",
        "\n",
        "    assert tokens.ndim == 2 and tokens.shape[0] == 1, \\\n",
        "        f\"Expected tokens of shape [1, L], got {tokens.shape}\"\n",
        "\n",
        "    # 1. Create empty store\n",
        "    store = make_attention_store()\n",
        "\n",
        "    # 2. Build hooks (one for each layer)\n",
        "    hooks = get_attention_hooks(model, store)\n",
        "\n",
        "    # 3. Run forward pass with hooks\n",
        "    with torch.no_grad():\n",
        "        _ = model.run_with_hooks(\n",
        "            tokens,\n",
        "            fwd_hooks=hooks\n",
        "        )\n",
        "\n",
        "    # 4. Post-process:\n",
        "    # Each entry is [1, n_heads, L, L] → squeeze batch dim to [n_heads, L, L]\n",
        "    final_store = {}\n",
        "    for layer_idx, attn_tensor in store.items():\n",
        "        final_store[layer_idx] = attn_tensor[0]  # remove batch dimension\n",
        "\n",
        "    return final_store\n"
      ],
      "metadata": {
        "id": "7GQFdMAk5hTV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokenized_example(path: str):\n",
        "    \"\"\"\n",
        "    Returns the first non-empty JSON line as a Python dict.\n",
        "    \"\"\"\n",
        "    with open(path, \"r\") as f:\n",
        "        try:\n",
        "            data = json.load(f)  # Load the entire JSON array\n",
        "            if data:\n",
        "                return data[0]  # Return the first example\n",
        "            else:\n",
        "                raise ValueError(\"The JSON array is empty.\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON: {e}\")\n",
        "            return None\n",
        "\n",
        "# Assuming you've loaded the correct row using the function above\n",
        "row = load_tokenized_example(TOKENIZED_PATH)\n",
        "\n",
        "# Make sure row is a valid dictionary before processing\n",
        "if isinstance(row, dict):\n",
        "    # Run the attention extraction and checks\n",
        "    attn_store = run_and_get_attention(model, row)\n",
        "\n",
        "    # How many layers did we capture?\n",
        "    layer_indices = sorted(attn_store.keys())\n",
        "    print(\"Layers captured:\", len(layer_indices))\n",
        "    print(\"Model reports n_layers =\", model.cfg.n_layers)\n",
        "\n",
        "    # Show shapes for the first couple of layers\n",
        "    for layer_idx in layer_indices[:2]:\n",
        "        attn_layer = attn_store[layer_idx]  # shape [n_heads, L, L]\n",
        "        print(f\"Layer {layer_idx} attention shape:\", attn_layer.shape)\n",
        "\n",
        "    # Row-sum sanity check for one head in one layer\n",
        "    some_layer = layer_indices[0]  # Select the first layer for the sanity check\n",
        "    attn_layer0 = attn_store[some_layer]    # Attention for the first layer, shape [n_heads, L, L]\n",
        "    head0 = attn_layer0[0]                  # Select the first head in this layer, shape [L, L]\n",
        "\n",
        "    # Sum along the last dimension (for each row) to verify normalization\n",
        "    row_sums = head0.sum(dim=-1)            # Sum along the last axis (columns), shape [L]\n",
        "\n",
        "    # Print the mean and standard deviation of row sums\n",
        "    print(f\"\\nRow sums for layer {some_layer}, head 0:\")\n",
        "    print(f\"  mean: {row_sums.mean().item():.6f}\")\n",
        "    print(f\"  std:  {row_sums.std().item():.6f}\")\n",
        "\n",
        "    print(\"\\nIf mean ≈ 1.0 and std is small, attention extraction is working.\")\n",
        "else:\n",
        "    print(\"Error: 'row' is not a valid dictionary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xa4mTpN4V7k",
        "outputId": "2a4cafe2-0f63-4119-fdc3-0b63c60bacd6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers captured: 32\n",
            "Model reports n_layers = 32\n",
            "Layer 0 attention shape: torch.Size([32, 22, 22])\n",
            "Layer 1 attention shape: torch.Size([32, 22, 22])\n",
            "\n",
            "Row sums for layer 0, head 0:\n",
            "  mean: 1.000000\n",
            "  std:  0.000000\n",
            "\n",
            "If mean ≈ 1.0 and std is small, attention extraction is working.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for hp in model.hook_points():\n",
        "    if \".attn.\" in hp.name:  # Check if the hook point involves attention\n",
        "        print(hp.name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuL7nHOh5nNv",
        "outputId": "4e499a9e-ab29-4e4c-9f2a-a8ebd41fec08",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blocks.0.attn.hook_k\n",
            "blocks.0.attn.hook_q\n",
            "blocks.0.attn.hook_v\n",
            "blocks.0.attn.hook_z\n",
            "blocks.0.attn.hook_attn_scores\n",
            "blocks.0.attn.hook_pattern\n",
            "blocks.0.attn.hook_result\n",
            "blocks.0.attn.hook_rot_k\n",
            "blocks.0.attn.hook_rot_q\n",
            "blocks.1.attn.hook_k\n",
            "blocks.1.attn.hook_q\n",
            "blocks.1.attn.hook_v\n",
            "blocks.1.attn.hook_z\n",
            "blocks.1.attn.hook_attn_scores\n",
            "blocks.1.attn.hook_pattern\n",
            "blocks.1.attn.hook_result\n",
            "blocks.1.attn.hook_rot_k\n",
            "blocks.1.attn.hook_rot_q\n",
            "blocks.2.attn.hook_k\n",
            "blocks.2.attn.hook_q\n",
            "blocks.2.attn.hook_v\n",
            "blocks.2.attn.hook_z\n",
            "blocks.2.attn.hook_attn_scores\n",
            "blocks.2.attn.hook_pattern\n",
            "blocks.2.attn.hook_result\n",
            "blocks.2.attn.hook_rot_k\n",
            "blocks.2.attn.hook_rot_q\n",
            "blocks.3.attn.hook_k\n",
            "blocks.3.attn.hook_q\n",
            "blocks.3.attn.hook_v\n",
            "blocks.3.attn.hook_z\n",
            "blocks.3.attn.hook_attn_scores\n",
            "blocks.3.attn.hook_pattern\n",
            "blocks.3.attn.hook_result\n",
            "blocks.3.attn.hook_rot_k\n",
            "blocks.3.attn.hook_rot_q\n",
            "blocks.4.attn.hook_k\n",
            "blocks.4.attn.hook_q\n",
            "blocks.4.attn.hook_v\n",
            "blocks.4.attn.hook_z\n",
            "blocks.4.attn.hook_attn_scores\n",
            "blocks.4.attn.hook_pattern\n",
            "blocks.4.attn.hook_result\n",
            "blocks.4.attn.hook_rot_k\n",
            "blocks.4.attn.hook_rot_q\n",
            "blocks.5.attn.hook_k\n",
            "blocks.5.attn.hook_q\n",
            "blocks.5.attn.hook_v\n",
            "blocks.5.attn.hook_z\n",
            "blocks.5.attn.hook_attn_scores\n",
            "blocks.5.attn.hook_pattern\n",
            "blocks.5.attn.hook_result\n",
            "blocks.5.attn.hook_rot_k\n",
            "blocks.5.attn.hook_rot_q\n",
            "blocks.6.attn.hook_k\n",
            "blocks.6.attn.hook_q\n",
            "blocks.6.attn.hook_v\n",
            "blocks.6.attn.hook_z\n",
            "blocks.6.attn.hook_attn_scores\n",
            "blocks.6.attn.hook_pattern\n",
            "blocks.6.attn.hook_result\n",
            "blocks.6.attn.hook_rot_k\n",
            "blocks.6.attn.hook_rot_q\n",
            "blocks.7.attn.hook_k\n",
            "blocks.7.attn.hook_q\n",
            "blocks.7.attn.hook_v\n",
            "blocks.7.attn.hook_z\n",
            "blocks.7.attn.hook_attn_scores\n",
            "blocks.7.attn.hook_pattern\n",
            "blocks.7.attn.hook_result\n",
            "blocks.7.attn.hook_rot_k\n",
            "blocks.7.attn.hook_rot_q\n",
            "blocks.8.attn.hook_k\n",
            "blocks.8.attn.hook_q\n",
            "blocks.8.attn.hook_v\n",
            "blocks.8.attn.hook_z\n",
            "blocks.8.attn.hook_attn_scores\n",
            "blocks.8.attn.hook_pattern\n",
            "blocks.8.attn.hook_result\n",
            "blocks.8.attn.hook_rot_k\n",
            "blocks.8.attn.hook_rot_q\n",
            "blocks.9.attn.hook_k\n",
            "blocks.9.attn.hook_q\n",
            "blocks.9.attn.hook_v\n",
            "blocks.9.attn.hook_z\n",
            "blocks.9.attn.hook_attn_scores\n",
            "blocks.9.attn.hook_pattern\n",
            "blocks.9.attn.hook_result\n",
            "blocks.9.attn.hook_rot_k\n",
            "blocks.9.attn.hook_rot_q\n",
            "blocks.10.attn.hook_k\n",
            "blocks.10.attn.hook_q\n",
            "blocks.10.attn.hook_v\n",
            "blocks.10.attn.hook_z\n",
            "blocks.10.attn.hook_attn_scores\n",
            "blocks.10.attn.hook_pattern\n",
            "blocks.10.attn.hook_result\n",
            "blocks.10.attn.hook_rot_k\n",
            "blocks.10.attn.hook_rot_q\n",
            "blocks.11.attn.hook_k\n",
            "blocks.11.attn.hook_q\n",
            "blocks.11.attn.hook_v\n",
            "blocks.11.attn.hook_z\n",
            "blocks.11.attn.hook_attn_scores\n",
            "blocks.11.attn.hook_pattern\n",
            "blocks.11.attn.hook_result\n",
            "blocks.11.attn.hook_rot_k\n",
            "blocks.11.attn.hook_rot_q\n",
            "blocks.12.attn.hook_k\n",
            "blocks.12.attn.hook_q\n",
            "blocks.12.attn.hook_v\n",
            "blocks.12.attn.hook_z\n",
            "blocks.12.attn.hook_attn_scores\n",
            "blocks.12.attn.hook_pattern\n",
            "blocks.12.attn.hook_result\n",
            "blocks.12.attn.hook_rot_k\n",
            "blocks.12.attn.hook_rot_q\n",
            "blocks.13.attn.hook_k\n",
            "blocks.13.attn.hook_q\n",
            "blocks.13.attn.hook_v\n",
            "blocks.13.attn.hook_z\n",
            "blocks.13.attn.hook_attn_scores\n",
            "blocks.13.attn.hook_pattern\n",
            "blocks.13.attn.hook_result\n",
            "blocks.13.attn.hook_rot_k\n",
            "blocks.13.attn.hook_rot_q\n",
            "blocks.14.attn.hook_k\n",
            "blocks.14.attn.hook_q\n",
            "blocks.14.attn.hook_v\n",
            "blocks.14.attn.hook_z\n",
            "blocks.14.attn.hook_attn_scores\n",
            "blocks.14.attn.hook_pattern\n",
            "blocks.14.attn.hook_result\n",
            "blocks.14.attn.hook_rot_k\n",
            "blocks.14.attn.hook_rot_q\n",
            "blocks.15.attn.hook_k\n",
            "blocks.15.attn.hook_q\n",
            "blocks.15.attn.hook_v\n",
            "blocks.15.attn.hook_z\n",
            "blocks.15.attn.hook_attn_scores\n",
            "blocks.15.attn.hook_pattern\n",
            "blocks.15.attn.hook_result\n",
            "blocks.15.attn.hook_rot_k\n",
            "blocks.15.attn.hook_rot_q\n",
            "blocks.16.attn.hook_k\n",
            "blocks.16.attn.hook_q\n",
            "blocks.16.attn.hook_v\n",
            "blocks.16.attn.hook_z\n",
            "blocks.16.attn.hook_attn_scores\n",
            "blocks.16.attn.hook_pattern\n",
            "blocks.16.attn.hook_result\n",
            "blocks.16.attn.hook_rot_k\n",
            "blocks.16.attn.hook_rot_q\n",
            "blocks.17.attn.hook_k\n",
            "blocks.17.attn.hook_q\n",
            "blocks.17.attn.hook_v\n",
            "blocks.17.attn.hook_z\n",
            "blocks.17.attn.hook_attn_scores\n",
            "blocks.17.attn.hook_pattern\n",
            "blocks.17.attn.hook_result\n",
            "blocks.17.attn.hook_rot_k\n",
            "blocks.17.attn.hook_rot_q\n",
            "blocks.18.attn.hook_k\n",
            "blocks.18.attn.hook_q\n",
            "blocks.18.attn.hook_v\n",
            "blocks.18.attn.hook_z\n",
            "blocks.18.attn.hook_attn_scores\n",
            "blocks.18.attn.hook_pattern\n",
            "blocks.18.attn.hook_result\n",
            "blocks.18.attn.hook_rot_k\n",
            "blocks.18.attn.hook_rot_q\n",
            "blocks.19.attn.hook_k\n",
            "blocks.19.attn.hook_q\n",
            "blocks.19.attn.hook_v\n",
            "blocks.19.attn.hook_z\n",
            "blocks.19.attn.hook_attn_scores\n",
            "blocks.19.attn.hook_pattern\n",
            "blocks.19.attn.hook_result\n",
            "blocks.19.attn.hook_rot_k\n",
            "blocks.19.attn.hook_rot_q\n",
            "blocks.20.attn.hook_k\n",
            "blocks.20.attn.hook_q\n",
            "blocks.20.attn.hook_v\n",
            "blocks.20.attn.hook_z\n",
            "blocks.20.attn.hook_attn_scores\n",
            "blocks.20.attn.hook_pattern\n",
            "blocks.20.attn.hook_result\n",
            "blocks.20.attn.hook_rot_k\n",
            "blocks.20.attn.hook_rot_q\n",
            "blocks.21.attn.hook_k\n",
            "blocks.21.attn.hook_q\n",
            "blocks.21.attn.hook_v\n",
            "blocks.21.attn.hook_z\n",
            "blocks.21.attn.hook_attn_scores\n",
            "blocks.21.attn.hook_pattern\n",
            "blocks.21.attn.hook_result\n",
            "blocks.21.attn.hook_rot_k\n",
            "blocks.21.attn.hook_rot_q\n",
            "blocks.22.attn.hook_k\n",
            "blocks.22.attn.hook_q\n",
            "blocks.22.attn.hook_v\n",
            "blocks.22.attn.hook_z\n",
            "blocks.22.attn.hook_attn_scores\n",
            "blocks.22.attn.hook_pattern\n",
            "blocks.22.attn.hook_result\n",
            "blocks.22.attn.hook_rot_k\n",
            "blocks.22.attn.hook_rot_q\n",
            "blocks.23.attn.hook_k\n",
            "blocks.23.attn.hook_q\n",
            "blocks.23.attn.hook_v\n",
            "blocks.23.attn.hook_z\n",
            "blocks.23.attn.hook_attn_scores\n",
            "blocks.23.attn.hook_pattern\n",
            "blocks.23.attn.hook_result\n",
            "blocks.23.attn.hook_rot_k\n",
            "blocks.23.attn.hook_rot_q\n",
            "blocks.24.attn.hook_k\n",
            "blocks.24.attn.hook_q\n",
            "blocks.24.attn.hook_v\n",
            "blocks.24.attn.hook_z\n",
            "blocks.24.attn.hook_attn_scores\n",
            "blocks.24.attn.hook_pattern\n",
            "blocks.24.attn.hook_result\n",
            "blocks.24.attn.hook_rot_k\n",
            "blocks.24.attn.hook_rot_q\n",
            "blocks.25.attn.hook_k\n",
            "blocks.25.attn.hook_q\n",
            "blocks.25.attn.hook_v\n",
            "blocks.25.attn.hook_z\n",
            "blocks.25.attn.hook_attn_scores\n",
            "blocks.25.attn.hook_pattern\n",
            "blocks.25.attn.hook_result\n",
            "blocks.25.attn.hook_rot_k\n",
            "blocks.25.attn.hook_rot_q\n",
            "blocks.26.attn.hook_k\n",
            "blocks.26.attn.hook_q\n",
            "blocks.26.attn.hook_v\n",
            "blocks.26.attn.hook_z\n",
            "blocks.26.attn.hook_attn_scores\n",
            "blocks.26.attn.hook_pattern\n",
            "blocks.26.attn.hook_result\n",
            "blocks.26.attn.hook_rot_k\n",
            "blocks.26.attn.hook_rot_q\n",
            "blocks.27.attn.hook_k\n",
            "blocks.27.attn.hook_q\n",
            "blocks.27.attn.hook_v\n",
            "blocks.27.attn.hook_z\n",
            "blocks.27.attn.hook_attn_scores\n",
            "blocks.27.attn.hook_pattern\n",
            "blocks.27.attn.hook_result\n",
            "blocks.27.attn.hook_rot_k\n",
            "blocks.27.attn.hook_rot_q\n",
            "blocks.28.attn.hook_k\n",
            "blocks.28.attn.hook_q\n",
            "blocks.28.attn.hook_v\n",
            "blocks.28.attn.hook_z\n",
            "blocks.28.attn.hook_attn_scores\n",
            "blocks.28.attn.hook_pattern\n",
            "blocks.28.attn.hook_result\n",
            "blocks.28.attn.hook_rot_k\n",
            "blocks.28.attn.hook_rot_q\n",
            "blocks.29.attn.hook_k\n",
            "blocks.29.attn.hook_q\n",
            "blocks.29.attn.hook_v\n",
            "blocks.29.attn.hook_z\n",
            "blocks.29.attn.hook_attn_scores\n",
            "blocks.29.attn.hook_pattern\n",
            "blocks.29.attn.hook_result\n",
            "blocks.29.attn.hook_rot_k\n",
            "blocks.29.attn.hook_rot_q\n",
            "blocks.30.attn.hook_k\n",
            "blocks.30.attn.hook_q\n",
            "blocks.30.attn.hook_v\n",
            "blocks.30.attn.hook_z\n",
            "blocks.30.attn.hook_attn_scores\n",
            "blocks.30.attn.hook_pattern\n",
            "blocks.30.attn.hook_result\n",
            "blocks.30.attn.hook_rot_k\n",
            "blocks.30.attn.hook_rot_q\n",
            "blocks.31.attn.hook_k\n",
            "blocks.31.attn.hook_q\n",
            "blocks.31.attn.hook_v\n",
            "blocks.31.attn.hook_z\n",
            "blocks.31.attn.hook_attn_scores\n",
            "blocks.31.attn.hook_pattern\n",
            "blocks.31.attn.hook_result\n",
            "blocks.31.attn.hook_rot_k\n",
            "blocks.31.attn.hook_rot_q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "TOKENIZED_PATH = \"/content/token_segmentation_metadata.json\"  # adjust if needed\n",
        "\n",
        "def load_tokenized_examples(path: str):\n",
        "    \"\"\"\n",
        "    Loads the entire JSON array from the file.\n",
        "    \"\"\"\n",
        "    with open(path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "examples = load_tokenized_examples(TOKENIZED_PATH)\n",
        "print(f\"Loaded {len(examples)} examples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VkE0Pm70vbq",
        "outputId": "ab1aefe6-e517-4d57-af5d-7cdcc8fa864a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 300 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row = examples[0]\n",
        "\n",
        "print(\"Keys:\", row.keys())\n",
        "print(\"Example id:\", row.get(\"id\"))\n",
        "print(\"constraint_tags:\", row.get(\"constraint_tags\"))\n",
        "\n",
        "input_ids = row[\"input_ids\"]\n",
        "p_span = row[\"p_span\"]   # or row[\"persona_span\"]\n",
        "u_span = row[\"u_span\"]\n",
        "a_span = row[\"a_span\"]\n",
        "\n",
        "L = len(input_ids)\n",
        "\n",
        "print(f\"seq_len (L): {L}\")\n",
        "print(\"p_span:\", p_span)\n",
        "print(\"u_span:\", u_span)\n",
        "print(\"a_span:\", a_span)\n",
        "\n",
        "p_len = p_span[1] - p_span[0] + 1\n",
        "u_len = u_span[1] - u_span[0] + 1\n",
        "a_len = a_span[1] - a_span[0] + 1\n",
        "\n",
        "print(f\"p_len={p_len}, u_len={u_len}, a_len={a_len}, sum={p_len + u_len + a_len}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_vFWBkG08lk",
        "outputId": "37f1a37b-78dc-4b81-add5-07748a55c3cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: dict_keys(['id', 'dataset', 'input_ids', 'p_span', 'u_span', 'a_span'])\n",
            "Example id: alpaca:22052\n",
            "constraint_tags: None\n",
            "seq_len (L): 22\n",
            "p_span: [0, 18]\n",
            "u_span: [20, 20]\n",
            "a_span: [22, 22]\n",
            "p_len=19, u_len=1, a_len=1, sum=21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "def make_attention_store():\n",
        "    return {}\n",
        "\n",
        "def save_attention_hook(attn, hook, store):\n",
        "    layer_idx = hook.layer()\n",
        "    # attn shape: [batch, n_heads, L, L]\n",
        "    store[layer_idx] = attn.detach().cpu()\n",
        "\n",
        "def get_attention_hooks(model, store):\n",
        "    hooks = []\n",
        "    all_hooks = {hp.name for hp in model.hook_points()}\n",
        "\n",
        "    for layer in range(model.cfg.n_layers):\n",
        "        hook_name = f\"blocks.{layer}.attn.hook_pattern\"\n",
        "        if hook_name not in all_hooks:\n",
        "            raise KeyError(\n",
        "                f\"{hook_name} not found in model.hook_points(). \"\n",
        "                f\"Available attention hooks include: \"\n",
        "                f\"{[h for h in all_hooks if 'attn' in h]}\"\n",
        "            )\n",
        "        hook_fn = partial(save_attention_hook, store=store)\n",
        "        hooks.append((hook_name, hook_fn))\n",
        "    return hooks\n"
      ],
      "metadata": {
        "id": "E5e7Nwzc1KDd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def run_and_get_attention(model, row):\n",
        "    \"\"\"\n",
        "    Given a tokenized row (with 'input_ids'), return:\n",
        "        dict[layer_idx] -> tensor [n_heads, L, L]\n",
        "    \"\"\"\n",
        "    input_ids = row[\"input_ids\"]\n",
        "    tokens = torch.tensor([input_ids], device=None, dtype=torch.long)  # [1, L]\n",
        "\n",
        "    store = make_attention_store()\n",
        "    hooks = get_attention_hooks(model, store)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model.run_with_hooks(tokens, fwd_hooks=hooks)\n",
        "\n",
        "    final_store = {}\n",
        "    for layer_idx, attn_tensor in store.items():\n",
        "        # attn_tensor: [1, n_heads, L, L]\n",
        "        final_store[layer_idx] = attn_tensor[0]  # [n_heads, L, L]\n",
        "\n",
        "    return final_store"
      ],
      "metadata": {
        "id": "AFey3yJp1PRA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_store = run_and_get_attention(model, row)\n",
        "\n",
        "print(\"Layers in attn_store:\", sorted(attn_store.keys()))\n",
        "layer0 = sorted(attn_store.keys())[0]\n",
        "print(\"Layer 0 attention shape:\", attn_store[layer0].shape)  # expect [n_heads, L, L]\n",
        "\n",
        "# Check softmax normalization: row sums ~1\n",
        "head0 = attn_store[layer0][0]  # [L, L]\n",
        "row_sums = head0.sum(dim=-1)\n",
        "print(\"Row sums mean:\", row_sums.mean().item())\n",
        "print(\"Row sums std:\", row_sums.std().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFhsG_ux1T2s",
        "outputId": "ee842797-288c-43f9-ea32-45c9f12d2ca8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers in attn_store: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
            "Layer 0 attention shape: torch.Size([32, 22, 22])\n",
            "Row sums mean: 1.0\n",
            "Row sums std: 3.4412757088375656e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract span indices\n",
        "\n",
        "def get_span_indices(row):\n",
        "    L = len(row[\"input_ids\"])\n",
        "\n",
        "    def clamp_span(span):\n",
        "        s, e = span\n",
        "        s = max(0, min(s, L-1))\n",
        "        e = max(0, min(e, L-1))\n",
        "        if e < s:\n",
        "            e = s\n",
        "        return list(range(s, e+1))\n",
        "\n",
        "    P = clamp_span(row[\"p_span\"])\n",
        "    U = clamp_span(row[\"u_span\"])\n",
        "    A = clamp_span(row[\"a_span\"])\n",
        "\n",
        "    return P, U, A\n",
        "\n",
        "\n",
        "P, U, A = get_span_indices(row)\n"
      ],
      "metadata": {
        "id": "3jQ-Yt3s20y5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def region_score(attn_head, from_idx, to_idx):\n",
        "    if len(from_idx) == 0 or len(to_idx) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    from_t = torch.tensor(from_idx, dtype=torch.long)\n",
        "    to_t = torch.tensor(to_idx, dtype=torch.long)\n",
        "\n",
        "    # submatrix: rows = FROM tokens, columns = TO tokens\n",
        "    sub = attn_head[from_t][:, to_t]       # [|FROM|, |TO|]\n",
        "    num = sub.sum().item()\n",
        "\n",
        "    total_from = attn_head[from_t].sum().item()  # ≈ |FROM| because rows sum to 1\n",
        "\n",
        "    return num / total_from if total_from > 0 else 0.0\n"
      ],
      "metadata": {
        "id": "yqSgqMWQ8Lsg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "def compute_layer_metrics(attn_layer, P, U, A):\n",
        "    \"\"\"\n",
        "    attn_layer: [n_heads, L, L]\n",
        "    \"\"\"\n",
        "\n",
        "    heads = []\n",
        "    for h in range(attn_layer.shape[0]):\n",
        "        head = attn_layer[h]  # [L, L]\n",
        "\n",
        "        pam = region_score(head, A, P)  # assistant → persona\n",
        "        qam = region_score(head, A, U)  # assistant → user\n",
        "        sam = region_score(head, A, A)  # assistant → assistant\n",
        "\n",
        "        heads.append({\n",
        "            \"head\": h,\n",
        "            \"PAM\": pam,\n",
        "            \"QAM\": qam,\n",
        "            \"SAM\": sam\n",
        "        })\n",
        "\n",
        "    # Layer-level aggregates\n",
        "    layer_pam = mean(h[\"PAM\"] for h in heads)\n",
        "    layer_qam = mean(h[\"QAM\"] for h in heads)\n",
        "    layer_sam = mean(h[\"SAM\"] for h in heads)\n",
        "\n",
        "    return {\n",
        "        \"PAM\": layer_pam,\n",
        "        \"QAM\": layer_qam,\n",
        "        \"SAM\": layer_sam,\n",
        "        \"heads\": heads,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ApcaxVxY8P_p"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the first layer\n",
        "\n",
        "P, U, A = get_span_indices(row)\n",
        "print(\"len P/U/A:\", len(P), len(U), len(A))\n",
        "\n",
        "layer0 = sorted(attn_store.keys())[0]      # you already used this above\n",
        "attn_layer0 = attn_store[layer0]           # [n_heads, L, L]\n",
        "\n",
        "metrics_layer0 = compute_layer_metrics(attn_layer0, P, U, A)\n",
        "\n",
        "print(\"Layer 0 — PAM/QAM/SAM (averaged over heads):\")\n",
        "print(metrics_layer0[\"PAM\"], metrics_layer0[\"QAM\"], metrics_layer0[\"SAM\"])\n",
        "\n",
        "print(\"\\nFirst few heads:\")\n",
        "for h in metrics_layer0[\"heads\"][:5]:\n",
        "    print(h)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPt4Xbju8hGk",
        "outputId": "ee2c54fa-9cb9-4a7e-8abc-6e8b5e7b3516"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len P/U/A: 19 1 1\n",
            "Layer 0 — PAM/QAM/SAM (averaged over heads):\n",
            "0.6838161382990233 0.09756367403000328 0.1226047438733739\n",
            "\n",
            "First few heads:\n",
            "{'head': 0, 'PAM': 0.5002431273460388, 'QAM': 0.16298165917396545, 'SAM': 0.15198931097984314}\n",
            "{'head': 1, 'PAM': 0.00022489417460747063, 'QAM': 0.14286160469055176, 'SAM': 0.8524389863014221}\n",
            "{'head': 2, 'PAM': 0.5554549694061279, 'QAM': 0.14663861691951752, 'SAM': 0.13089054822921753}\n",
            "{'head': 3, 'PAM': 0.7453128099441528, 'QAM': 0.08623126149177551, 'SAM': 0.0899389386177063}\n",
            "{'head': 4, 'PAM': 0.1423267275094986, 'QAM': 0.34588828682899475, 'SAM': 0.009316587820649147}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get spans once for this row\n",
        "P, U, A = get_span_indices(row)\n",
        "\n",
        "# Compute all the layers\n",
        "all_layer_metrics = []\n",
        "\n",
        "for layer_idx in sorted(attn_store.keys()):\n",
        "    attn_layer = attn_store[layer_idx]   # [n_heads, L, L]\n",
        "    metrics = compute_layer_metrics(attn_layer, P, U, A)\n",
        "\n",
        "    all_layer_metrics.append({\n",
        "        \"layer\": int(layer_idx),\n",
        "        \"PAM\": metrics[\"PAM\"],\n",
        "        \"QAM\": metrics[\"QAM\"],\n",
        "        \"SAM\": metrics[\"SAM\"],\n",
        "        \"heads\": metrics[\"heads\"],\n",
        "    })\n",
        "\n",
        "# Quick preview\n",
        "print(\"Number of layers:\", len(all_layer_metrics))\n",
        "print(\"Layer 0 metrics summary:\", all_layer_metrics[0])\n",
        "print(\"Layer 1 metrics summary:\", all_layer_metrics[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2oINpuZ8kmJ",
        "outputId": "8e40887d-3551-4226-d165-f469381378b3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers: 32\n",
            "Layer 0 metrics summary: {'layer': 0, 'PAM': 0.6838161382990233, 'QAM': 0.09756367403000328, 'SAM': 0.1226047438733739, 'heads': [{'head': 0, 'PAM': 0.5002431273460388, 'QAM': 0.16298165917396545, 'SAM': 0.15198931097984314}, {'head': 1, 'PAM': 0.00022489417460747063, 'QAM': 0.14286160469055176, 'SAM': 0.8524389863014221}, {'head': 2, 'PAM': 0.5554549694061279, 'QAM': 0.14663861691951752, 'SAM': 0.13089054822921753}, {'head': 3, 'PAM': 0.7453128099441528, 'QAM': 0.08623126149177551, 'SAM': 0.0899389386177063}, {'head': 4, 'PAM': 0.1423267275094986, 'QAM': 0.34588828682899475, 'SAM': 0.009316587820649147}, {'head': 5, 'PAM': 0.7184080481529236, 'QAM': 0.09277660399675369, 'SAM': 0.09983798861503601}, {'head': 6, 'PAM': 0.9020717740058899, 'QAM': 0.032831061631441116, 'SAM': 0.032264720648527145}, {'head': 7, 'PAM': 0.7081997990608215, 'QAM': 0.09755054861307144, 'SAM': 0.08535626530647278}, {'head': 8, 'PAM': 0.859629980303644, 'QAM': 0.041041376652799645, 'SAM': 0.03358140415438438}, {'head': 9, 'PAM': 0.808094013219715, 'QAM': 0.051335263331846195, 'SAM': 0.028835970764516043}, {'head': 10, 'PAM': 0.904424786567688, 'QAM': 0.031538087874650955, 'SAM': 0.03162705525755882}, {'head': 11, 'PAM': 0.7036557197570801, 'QAM': 0.10666943341493607, 'SAM': 0.08763308823108673}, {'head': 12, 'PAM': 0.537945541020962, 'QAM': 0.15635592677330534, 'SAM': 0.15978758989498554}, {'head': 13, 'PAM': 0.7532569169998169, 'QAM': 0.08202837407588959, 'SAM': 0.07698685675859451}, {'head': 14, 'PAM': 0.4980808200739837, 'QAM': 0.16339097459423846, 'SAM': 0.14873654857438223}, {'head': 15, 'PAM': 0.7677866497294128, 'QAM': 0.07833473940673596, 'SAM': 0.0759141652686399}, {'head': 16, 'PAM': 0.7070751190185547, 'QAM': 0.09366011619567871, 'SAM': 0.1394658088684082}, {'head': 17, 'PAM': 0.9977402091026306, 'QAM': 0.0007314911927096546, 'SAM': 0.0006019111024215817}, {'head': 18, 'PAM': 0.985055803361881, 'QAM': 0.004994973838029732, 'SAM': 0.004843354048183802}, {'head': 19, 'PAM': 0.7823442220687866, 'QAM': 0.07204796373844147, 'SAM': 0.06402858346700668}, {'head': 20, 'PAM': 0.1347957563156791, 'QAM': 0.25767889710413183, 'SAM': 0.5021475556072659}, {'head': 21, 'PAM': 0.8777134345599076, 'QAM': 0.03997583180521916, 'SAM': 0.03667616317726154}, {'head': 22, 'PAM': 0.6582624708570523, 'QAM': 0.10385180138658294, 'SAM': 0.1548529955657122}, {'head': 23, 'PAM': 0.6326564988307358, 'QAM': 0.10965618674085298, 'SAM': 0.20565768114832864}, {'head': 24, 'PAM': 0.8090659038542337, 'QAM': 0.06473642255123065, 'SAM': 0.08497997402786744}, {'head': 25, 'PAM': 0.601735532283783, 'QAM': 0.1335306316614151, 'SAM': 0.14020946621894836}, {'head': 26, 'PAM': 0.6445364316469011, 'QAM': 0.0865589162514656, 'SAM': 0.02540152229097884}, {'head': 27, 'PAM': 0.604052590840746, 'QAM': 0.11375973507645903, 'SAM': 0.22590117443785973}, {'head': 28, 'PAM': 0.709543764591217, 'QAM': 0.09723152965307236, 'SAM': 0.09859749674797058}, {'head': 29, 'PAM': 0.9691977500915527, 'QAM': 0.010337663814425468, 'SAM': 0.01018241886049509}, {'head': 30, 'PAM': 0.6713180746219247, 'QAM': 0.11217065002568365, 'SAM': 0.13189542375748442}, {'head': 31, 'PAM': 0.9919062862507956, 'QAM': 0.002660938454232996, 'SAM': 0.002774249198749757}]}\n",
            "Layer 1 metrics summary: {'layer': 1, 'PAM': 0.709832681980253, 'QAM': 0.09913443393878295, 'SAM': 0.08789292400931963, 'heads': [{'head': 0, 'PAM': 0.2969236550881657, 'QAM': 0.2554017159582207, 'SAM': 0.19704143387326203}, {'head': 1, 'PAM': 0.8141890168190002, 'QAM': 0.06068461015820503, 'SAM': 0.05515049025416374}, {'head': 2, 'PAM': 0.7086857210023071, 'QAM': 0.09359185768268796, 'SAM': 0.09361581130216257}, {'head': 3, 'PAM': 0.8356434008862615, 'QAM': 0.05288332643409529, 'SAM': 0.05001616031027796}, {'head': 4, 'PAM': 0.7945202466559557, 'QAM': 0.06728491588144993, 'SAM': 0.06434653337875207}, {'head': 5, 'PAM': 0.8058169484138489, 'QAM': 0.06423039734363556, 'SAM': 0.0659414604306221}, {'head': 6, 'PAM': 0.7096503799945343, 'QAM': 0.09656701812547554, 'SAM': 0.0949723330123623}, {'head': 7, 'PAM': 0.6727628111839294, 'QAM': 0.10842578113079071, 'SAM': 0.09613247960805893}, {'head': 8, 'PAM': 0.7877464294433594, 'QAM': 0.06823201477527618, 'SAM': 0.059252575039863586}, {'head': 9, 'PAM': 0.8693377893768424, 'QAM': 0.04058371353648386, 'SAM': 0.02866056546035799}, {'head': 10, 'PAM': 0.7120001316070557, 'QAM': 0.08638108521699905, 'SAM': 0.050785765051841736}, {'head': 11, 'PAM': 0.7976704481040897, 'QAM': 0.06599155771832971, 'SAM': 0.060634743644625705}, {'head': 12, 'PAM': 0.8640540838241577, 'QAM': 0.04524736851453781, 'SAM': 0.04375118762254715}, {'head': 13, 'PAM': 0.7130988836288452, 'QAM': 0.09394373744726181, 'SAM': 0.09336720407009125}, {'head': 14, 'PAM': 0.8059052709284587, 'QAM': 0.06393470251171007, 'SAM': 0.062359701833707204}, {'head': 15, 'PAM': 0.620625532902809, 'QAM': 0.1355731121047206, 'SAM': 0.12135667332152565}, {'head': 16, 'PAM': 0.6977592526530774, 'QAM': 0.10275755690083246, 'SAM': 0.08571626011826158}, {'head': 17, 'PAM': 0.825307330452522, 'QAM': 0.05616074503532111, 'SAM': 0.038607328253110855}, {'head': 18, 'PAM': 0.8757981061935425, 'QAM': 0.038667768239974976, 'SAM': 0.026818381622433662}, {'head': 19, 'PAM': 0.43665331224520876, 'QAM': 0.20960205552590225, 'SAM': 0.185690175634037}, {'head': 20, 'PAM': 0.8112999200820923, 'QAM': 0.0613919161260128, 'SAM': 0.05831461027264595}, {'head': 21, 'PAM': 0.850801408290863, 'QAM': 0.04841452091932297, 'SAM': 0.047064270824193954}, {'head': 22, 'PAM': 0.17019768598052096, 'QAM': 0.32221249606758307, 'SAM': 0.33782578561613386}, {'head': 23, 'PAM': 0.8286009924769994, 'QAM': 0.053558978650509036, 'SAM': 0.03711395559394095}, {'head': 24, 'PAM': 0.7244424552390033, 'QAM': 0.09396510196148133, 'SAM': 0.0942262820331714}, {'head': 25, 'PAM': 0.5760722160339355, 'QAM': 0.1387055218219757, 'SAM': 0.07588429749011993}, {'head': 26, 'PAM': 0.6748423963973049, 'QAM': 0.10340774316695414, 'SAM': 0.06728068682185569}, {'head': 27, 'PAM': 0.4995551109313965, 'QAM': 0.18942226469516754, 'SAM': 0.16746056079864502}, {'head': 28, 'PAM': 0.7014018893241882, 'QAM': 0.1039656549692154, 'SAM': 0.10061463713645935}, {'head': 29, 'PAM': 0.6393587374304972, 'QAM': 0.11967292396264816, 'SAM': 0.12852332762022778}, {'head': 30, 'PAM': 0.7902952307638663, 'QAM': 0.06661640504696399, 'SAM': 0.062360741189762425}, {'head': 31, 'PAM': 0.8036290290134567, 'QAM': 0.06482331841130963, 'SAM': 0.06168714905900652}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the full JSON\n",
        "\n",
        "import json\n",
        "\n",
        "def build_example_record(row, attn_store):\n",
        "    # spans + lengths\n",
        "    P, U, A = get_span_indices(row)\n",
        "    L = len(row[\"input_ids\"])\n",
        "\n",
        "    layers_list = []\n",
        "    for layer_idx in sorted(attn_store.keys()):\n",
        "        metrics = compute_layer_metrics(attn_store[layer_idx], P, U, A)\n",
        "        layers_list.append({\n",
        "            \"layer\": int(layer_idx),\n",
        "            \"PAM\": metrics[\"PAM\"],\n",
        "            \"QAM\": metrics[\"QAM\"],\n",
        "            \"SAM\": metrics[\"SAM\"],\n",
        "            \"heads\": metrics[\"heads\"],\n",
        "        })\n",
        "\n",
        "    record = {\n",
        "        \"id\": row.get(\"id\"),\n",
        "        \"dataset\": row.get(\"dataset\", \"sharegpt\"),\n",
        "        \"constraint_tags\": row.get(\"constraint_tags\", []),\n",
        "        \"seq_len\": L,\n",
        "        \"p_len\": len(P),\n",
        "        \"u_len\": len(U),\n",
        "        \"a_len\": len(A),\n",
        "        \"layers\": layers_list,\n",
        "    }\n",
        "    return record\n",
        "\n",
        "# Build and inspect for this row\n",
        "record0 = build_example_record(row, attn_store)\n",
        "\n",
        "print(json.dumps(record0, indent=2)[:1500], \"...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EvXNj1fW9Xb7",
        "outputId": "6358fced-66f1-4c2c-e81f-cef6dee587c2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"alpaca:22052\",\n",
            "  \"dataset\": \"alpaca\",\n",
            "  \"constraint_tags\": [],\n",
            "  \"seq_len\": 22,\n",
            "  \"p_len\": 19,\n",
            "  \"u_len\": 1,\n",
            "  \"a_len\": 1,\n",
            "  \"layers\": [\n",
            "    {\n",
            "      \"layer\": 0,\n",
            "      \"PAM\": 0.6838161382990233,\n",
            "      \"QAM\": 0.09756367403000328,\n",
            "      \"SAM\": 0.1226047438733739,\n",
            "      \"heads\": [\n",
            "        {\n",
            "          \"head\": 0,\n",
            "          \"PAM\": 0.5002431273460388,\n",
            "          \"QAM\": 0.16298165917396545,\n",
            "          \"SAM\": 0.15198931097984314\n",
            "        },\n",
            "        {\n",
            "          \"head\": 1,\n",
            "          \"PAM\": 0.00022489417460747063,\n",
            "          \"QAM\": 0.14286160469055176,\n",
            "          \"SAM\": 0.8524389863014221\n",
            "        },\n",
            "        {\n",
            "          \"head\": 2,\n",
            "          \"PAM\": 0.5554549694061279,\n",
            "          \"QAM\": 0.14663861691951752,\n",
            "          \"SAM\": 0.13089054822921753\n",
            "        },\n",
            "        {\n",
            "          \"head\": 3,\n",
            "          \"PAM\": 0.7453128099441528,\n",
            "          \"QAM\": 0.08623126149177551,\n",
            "          \"SAM\": 0.0899389386177063\n",
            "        },\n",
            "        {\n",
            "          \"head\": 4,\n",
            "          \"PAM\": 0.1423267275094986,\n",
            "          \"QAM\": 0.34588828682899475,\n",
            "          \"SAM\": 0.009316587820649147\n",
            "        },\n",
            "        {\n",
            "          \"head\": 5,\n",
            "          \"PAM\": 0.7184080481529236,\n",
            "          \"QAM\": 0.09277660399675369,\n",
            "          \"SAM\": 0.09983798861503601\n",
            "        },\n",
            "        {\n",
            "          \"head\": 6,\n",
            "          \"PAM\": 0.9020717740058899,\n",
            "          \"QAM\": 0.032831061631441116,\n",
            "          \"SAM\": 0.032264720648527145\n",
            "        },\n",
            "        {\n",
            "          \"head\": 7,\n",
            "          \"PAM\": 0.7081997990608215,\n",
            "          \"QAM\": 0.09755054861307144,\n",
            "          \"SAM\": 0.085 ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def region_scores_batch(attn_layer, from_idx, to_idx):\n",
        "    \"\"\"\n",
        "    attn_layer: [batch, n_heads, L, L]\n",
        "    from_idx, to_idx: 1D LongTensors\n",
        "    Returns: [batch, n_heads] fraction of attention mass FROM -> TO.\n",
        "    \"\"\"\n",
        "    if len(from_idx) == 0 or len(to_idx) == 0:\n",
        "        return attn_layer.new_zeros(attn_layer.shape[0], attn_layer.shape[1])\n",
        "\n",
        "    sub = attn_layer[:, :, from_idx][:, :, :, to_idx]    # [B, H, |FROM|, |TO|]\n",
        "    num = sub.sum(dim=(-1, -2))                          # [B, H]\n",
        "\n",
        "    total_from = attn_layer[:, :, from_idx].sum(dim=(-1, -2))  # [B, H]\n",
        "    scores = num / (total_from + 1e-9)\n",
        "    return scores\n",
        "\n"
      ],
      "metadata": {
        "id": "7SkEi3Z99cj8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "def make_metric_hook(layer_idx, store, P, U, A):\n",
        "    \"\"\"\n",
        "    store[layer_idx] will become:\n",
        "      {\n",
        "        \"PAM\": <float>,\n",
        "        \"QAM\": <float>,\n",
        "        \"SAM\": <float>,\n",
        "        \"heads\": [\n",
        "          {\"head\": h, \"PAM\": float, \"QAM\": float, \"SAM\": float}, ...\n",
        "        ]\n",
        "      }\n",
        "    \"\"\"\n",
        "    def hook_fn(attn, hook):\n",
        "        # attn: [batch, n_heads, L, L] (batch=1)\n",
        "        pam = region_scores_batch(attn, A, P)[0]  # [n_heads]\n",
        "        qam = region_scores_batch(attn, A, U)[0]  # [n_heads]\n",
        "        sam = region_scores_batch(attn, A, A)[0]  # [n_heads]\n",
        "\n",
        "        pam_list = pam.detach().cpu().tolist()\n",
        "        qam_list = qam.detach().cpu().tolist()\n",
        "        sam_list = sam.detach().cpu().tolist()\n",
        "\n",
        "        heads = []\n",
        "        for h, (p, q, s) in enumerate(zip(pam_list, qam_list, sam_list)):\n",
        "            heads.append({\"head\": h, \"PAM\": p, \"QAM\": q, \"SAM\": s})\n",
        "\n",
        "        layer_pam = float(sum(pam_list) / len(pam_list))\n",
        "        layer_qam = float(sum(qam_list) / len(qam_list))\n",
        "        layer_sam = float(sum(sam_list) / len(sam_list))\n",
        "\n",
        "        store[layer_idx] = {\n",
        "            \"PAM\": layer_pam,\n",
        "            \"QAM\": layer_qam,\n",
        "            \"SAM\": layer_sam,\n",
        "            \"heads\": heads,\n",
        "        }\n",
        "    return hook_fn\n"
      ],
      "metadata": {
        "id": "YOiwBUrdA43n"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_and_get_metrics(model, row, device=None):\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    input_ids = torch.tensor([row[\"input_ids\"]], device=device)\n",
        "\n",
        "    P_idx, U_idx, A_idx = get_span_indices(row)\n",
        "    P = torch.tensor(P_idx, device=device, dtype=torch.long)\n",
        "    U = torch.tensor(U_idx, device=device, dtype=torch.long)\n",
        "    A = torch.tensor(A_idx, device=device, dtype=torch.long)\n",
        "\n",
        "    store = {}\n",
        "\n",
        "    all_hooks = {hp.name for hp in model.hook_points()}\n",
        "    fwd_hooks = []\n",
        "    for layer in range(model.cfg.n_layers):\n",
        "        hook_name = f\"blocks.{layer}.attn.hook_pattern\"\n",
        "        if hook_name not in all_hooks:\n",
        "            raise KeyError(f\"{hook_name} not in hook_points\")\n",
        "        hook_fn = make_metric_hook(layer, store, P, U, A)\n",
        "        fwd_hooks.append((hook_name, hook_fn))\n",
        "\n",
        "    # inference_mode uses less memory than no_grad\n",
        "    with torch.inference_mode():\n",
        "        _ = model.run_with_hooks(input_ids, fwd_hooks=fwd_hooks)\n",
        "\n",
        "    return store\n"
      ],
      "metadata": {
        "id": "BRQrQUBrA67X"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_example_record_from_store(row, layer_store):\n",
        "    P_idx, U_idx, A_idx = get_span_indices(row)\n",
        "    L = len(row[\"input_ids\"])\n",
        "\n",
        "    layers_list = []\n",
        "    for layer_idx in sorted(layer_store.keys()):\n",
        "        lm = layer_store[layer_idx]\n",
        "        layers_list.append({\n",
        "            \"layer\": int(layer_idx),\n",
        "            \"PAM\": lm[\"PAM\"],\n",
        "            \"QAM\": lm[\"QAM\"],\n",
        "            \"SAM\": lm[\"SAM\"],\n",
        "            \"heads\": lm[\"heads\"],\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"id\": row.get(\"id\"),\n",
        "        \"dataset\": row.get(\"dataset\", \"sharegpt\"),\n",
        "        \"constraint_tags\": row.get(\"constraint_tags\", []),\n",
        "        \"seq_len\": L,\n",
        "        \"p_len\": len(P_idx),\n",
        "        \"u_len\": len(U_idx),\n",
        "        \"a_len\": len(A_idx),\n",
        "        \"layers\": layers_list,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "wtKjDTIpA86Z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_store = run_and_get_metrics(model, row)\n",
        "record0 = build_example_record_from_store(row, metrics_store)\n",
        "\n",
        "import json\n",
        "print(json.dumps(record0, indent=2)[:1500], \"...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AB7cU3yRA_Wp",
        "outputId": "8af11d7c-0e18-4d7b-a952-d8d530eaf6ab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"alpaca:22052\",\n",
            "  \"dataset\": \"alpaca\",\n",
            "  \"constraint_tags\": [],\n",
            "  \"seq_len\": 22,\n",
            "  \"p_len\": 19,\n",
            "  \"u_len\": 1,\n",
            "  \"a_len\": 1,\n",
            "  \"layers\": [\n",
            "    {\n",
            "      \"layer\": 0,\n",
            "      \"PAM\": 0.6838161504788332,\n",
            "      \"QAM\": 0.09756367398767907,\n",
            "      \"SAM\": 0.12260474392314791,\n",
            "      \"heads\": [\n",
            "        {\n",
            "          \"head\": 0,\n",
            "          \"PAM\": 0.5002431869506836,\n",
            "          \"QAM\": 0.16298165917396545,\n",
            "          \"SAM\": 0.15198931097984314\n",
            "        },\n",
            "        {\n",
            "          \"head\": 1,\n",
            "          \"PAM\": 0.0002248941600555554,\n",
            "          \"QAM\": 0.14286160469055176,\n",
            "          \"SAM\": 0.8524389863014221\n",
            "        },\n",
            "        {\n",
            "          \"head\": 2,\n",
            "          \"PAM\": 0.5554550290107727,\n",
            "          \"QAM\": 0.14663861691951752,\n",
            "          \"SAM\": 0.13089054822921753\n",
            "        },\n",
            "        {\n",
            "          \"head\": 3,\n",
            "          \"PAM\": 0.7453128099441528,\n",
            "          \"QAM\": 0.08623126149177551,\n",
            "          \"SAM\": 0.0899389386177063\n",
            "        },\n",
            "        {\n",
            "          \"head\": 4,\n",
            "          \"PAM\": 0.1423267275094986,\n",
            "          \"QAM\": 0.34588828682899475,\n",
            "          \"SAM\": 0.009316587820649147\n",
            "        },\n",
            "        {\n",
            "          \"head\": 5,\n",
            "          \"PAM\": 0.7184081673622131,\n",
            "          \"QAM\": 0.09277661144733429,\n",
            "          \"SAM\": 0.09983799606561661\n",
            "        },\n",
            "        {\n",
            "          \"head\": 6,\n",
            "          \"PAM\": 0.9020718336105347,\n",
            "          \"QAM\": 0.032831065356731415,\n",
            "          \"SAM\": 0.032264724373817444\n",
            "        },\n",
            "        {\n",
            "          \"head\": 7,\n",
            "          \"PAM\": 0.7081997394561768,\n",
            "          \"QAM\": 0.09755054861307144,\n",
            "          \"SAM\": 0.085 ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CTX = model.cfg.n_ctx  # should be 4096 for Llama-2\n",
        "print(\"Model max context:\", MAX_CTX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjGVgO7pB9IB",
        "outputId": "ddbd76bb-8bd4-4c1a-8c45-54c6f222a630"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model max context: 4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time, gc, torch\n",
        "\n",
        "OUTPUT_PATH = \"attention_metrics.jsonl\"\n",
        "\n",
        "MAX_CTX = model.cfg.n_ctx          # still keep this\n",
        "print(\"Model n_ctx:\", MAX_CTX)\n",
        "\n",
        "start = time.time()\n",
        "processed = 0\n",
        "skipped_long = 0\n",
        "skipped_oom = 0\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\") as f:\n",
        "    for i, ex in enumerate(examples):\n",
        "        L = len(ex[\"input_ids\"])\n",
        "\n",
        "        # 1) skip sequences that exceed model context\n",
        "        if L > MAX_CTX:\n",
        "            skipped_long += 1\n",
        "            if skipped_long <= 10:\n",
        "                print(f\"Skipping example {i} (len={L} > MAX_CTX={MAX_CTX})\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # 2) try full-length analysis\n",
        "            metrics_store = run_and_get_metrics(model, ex)\n",
        "            record = build_example_record_from_store(ex, metrics_store)\n",
        "            f.write(json.dumps(record) + \"\\n\")\n",
        "            processed += 1\n",
        "\n",
        "            del metrics_store, record\n",
        "            gc.collect()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"CUDA out of memory\" in str(e):\n",
        "                skipped_oom += 1\n",
        "                print(f\"OOM on example {i} (len={L}) – skipping. \"\n",
        "                      f\"OOMS so far: {skipped_oom}\")\n",
        "                torch.cuda.empty_cache()\n",
        "                continue\n",
        "            else:\n",
        "                # if it's some other error, re-raise\n",
        "                raise\n",
        "\n",
        "        if (processed + skipped_long + skipped_oom) % 50 == 0:\n",
        "            print(f\"{processed + skipped_long + skipped_oom}/{len(examples)} processed \"\n",
        "                  f\"(kept {processed}, skipped_long {skipped_long}, skipped_oom {skipped_oom})\")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f\"\\nDone. Kept {processed}, skipped_long {skipped_long}, \"\n",
        "      f\"skipped_oom {skipped_oom} in {elapsed:.1f}s \"\n",
        "      f\"(~{elapsed/max(processed,1):.3f}s per kept example)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kjD2Ep7B_WC",
        "outputId": "bbe52465-0937-4ad4-8c9b-fc096c851db7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model n_ctx: 4096\n",
            "OOM on example 2 (len=3635) – skipping. OOMS so far: 1\n",
            "Skipping example 6 (len=24291 > MAX_CTX=4096)\n",
            "Skipping example 14 (len=10750 > MAX_CTX=4096)\n",
            "Skipping example 15 (len=9342 > MAX_CTX=4096)\n",
            "Skipping example 18 (len=7653 > MAX_CTX=4096)\n",
            "Skipping example 21 (len=6829 > MAX_CTX=4096)\n",
            "OOM on example 27 (len=3493) – skipping. OOMS so far: 2\n",
            "Skipping example 34 (len=4221 > MAX_CTX=4096)\n",
            "OOM on example 42 (len=3605) – skipping. OOMS so far: 3\n",
            "Skipping example 48 (len=5533 > MAX_CTX=4096)\n",
            "50/300 processed (kept 40, skipped_long 7, skipped_oom 3)\n",
            "OOM on example 50 (len=3332) – skipping. OOMS so far: 4\n",
            "Skipping example 78 (len=9460 > MAX_CTX=4096)\n",
            "Skipping example 96 (len=7953 > MAX_CTX=4096)\n",
            "100/300 processed (kept 87, skipped_long 9, skipped_oom 4)\n",
            "Skipping example 102 (len=6303 > MAX_CTX=4096)\n",
            "OOM on example 114 (len=2695) – skipping. OOMS so far: 5\n",
            "OOM on example 121 (len=3668) – skipping. OOMS so far: 6\n",
            "150/300 processed (kept 132, skipped_long 12, skipped_oom 6)\n",
            "OOM on example 161 (len=3272) – skipping. OOMS so far: 7\n",
            "OOM on example 193 (len=3224) – skipping. OOMS so far: 8\n",
            "200/300 processed (kept 174, skipped_long 18, skipped_oom 8)\n",
            "OOM on example 228 (len=3559) – skipping. OOMS so far: 9\n",
            "OOM on example 231 (len=2645) – skipping. OOMS so far: 10\n",
            "250/300 processed (kept 220, skipped_long 20, skipped_oom 10)\n",
            "OOM on example 276 (len=3285) – skipping. OOMS so far: 11\n",
            "300/300 processed (kept 266, skipped_long 23, skipped_oom 11)\n",
            "\n",
            "Done. Kept 266, skipped_long 23, skipped_oom 11 in 222.9s (~0.838s per kept example)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IsiE5_zIG8ni"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "name": "Model_Attention_and_Extractionfixed.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "858a07b040a54909b8f601abbc548d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d176165ef905497f827dafb8d2200dc6"
            ],
            "layout": "IPY_MODEL_d98123303a654fa8a22c8196403674dd"
          }
        },
        "b24b241020b247528322700f9610b648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42960177390b48e6a4d01bff5d2860a9",
            "placeholder": "​",
            "style": "IPY_MODEL_f185a732d9a647a5b70afd349d0c6c6a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "c35586858782404f866948a7393ee9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ad38c1f582cb40bda37be292c7f5a09f",
            "placeholder": "​",
            "style": "IPY_MODEL_edeaf1d8e37d406f9726d98407712e6f",
            "value": ""
          }
        },
        "8fcc609d8bfc4efcb205637a6d937ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_dde084764c4e424a8d8dfa4383f4a7c1",
            "style": "IPY_MODEL_b628b5a3b9cf4e0c9c343235227291dc",
            "value": true
          }
        },
        "664809668490464d8157d176a9fc905b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3e7751039060425395fbe46d173b26b0",
            "style": "IPY_MODEL_9d2c0df19e6c4a11b31e6e326a565908",
            "tooltip": ""
          }
        },
        "5e9afe5342a04643a03eed6630300acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3cd64a302404b6fad352491e8159a88",
            "placeholder": "​",
            "style": "IPY_MODEL_b8871ff7e13d4a9894cc74090bbaf6c6",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d98123303a654fa8a22c8196403674dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "42960177390b48e6a4d01bff5d2860a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f185a732d9a647a5b70afd349d0c6c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad38c1f582cb40bda37be292c7f5a09f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edeaf1d8e37d406f9726d98407712e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dde084764c4e424a8d8dfa4383f4a7c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b628b5a3b9cf4e0c9c343235227291dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e7751039060425395fbe46d173b26b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d2c0df19e6c4a11b31e6e326a565908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f3cd64a302404b6fad352491e8159a88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8871ff7e13d4a9894cc74090bbaf6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ec607bbcafa47fd9438bfef6fb217ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5e48d6098fd4d2c855303d879044a6d",
            "placeholder": "​",
            "style": "IPY_MODEL_8477fe2d711643319e7038c40b32b154",
            "value": "Connecting..."
          }
        },
        "c5e48d6098fd4d2c855303d879044a6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8477fe2d711643319e7038c40b32b154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81621ea05741480f9da53bfae0c8d716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a7f6493831443ffbd5616c0b523253d",
            "placeholder": "​",
            "style": "IPY_MODEL_a25621b27b8b4368b1f35a47eddf3f32",
            "value": "Invalid user token."
          }
        },
        "8a7f6493831443ffbd5616c0b523253d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a25621b27b8b4368b1f35a47eddf3f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c90fb3045c54b19b98178b9d549332c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ab3b0d8d1444998be82061695408880",
            "placeholder": "​",
            "style": "IPY_MODEL_581acf062756404c98aebfbf9c7bfb8f",
            "value": "Connecting..."
          }
        },
        "5ab3b0d8d1444998be82061695408880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "581acf062756404c98aebfbf9c7bfb8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d176165ef905497f827dafb8d2200dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ea336e2fbac45bfaf5c38ad3258bf95",
            "placeholder": "​",
            "style": "IPY_MODEL_25d8f941ad9d4e5e84ccfea08b5fae78",
            "value": "Token HF_TOKEN not found in /root/.cache/huggingface/stored_tokens"
          }
        },
        "3ea336e2fbac45bfaf5c38ad3258bf95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25d8f941ad9d4e5e84ccfea08b5fae78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70205dc4efe841d495e1b6778075b50c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dceb40fbea13489fbeebb555d8946659",
              "IPY_MODEL_4c17c991f65d436e8183dd778cd2f1f4",
              "IPY_MODEL_1ad42372ad2e4cdbb9f561b5a9aa522c"
            ],
            "layout": "IPY_MODEL_dffaaf9f49d449ea87c38128c87f4fdd"
          }
        },
        "dceb40fbea13489fbeebb555d8946659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0990eee9a1e447bbed771078ddfd602",
            "placeholder": "​",
            "style": "IPY_MODEL_e83743f167324db3925f09d5adf23847",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4c17c991f65d436e8183dd778cd2f1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c26366d099b44c4f9edc041e554e0857",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ef1a2b31eb543f890e093156c79be19",
            "value": 2
          }
        },
        "1ad42372ad2e4cdbb9f561b5a9aa522c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f7fb3d60ba4b649d0f64791dd4d6d9",
            "placeholder": "​",
            "style": "IPY_MODEL_db0c9aa00e3c468eaeb37c3411a5f74d",
            "value": " 2/2 [00:00&lt;00:00,  3.75it/s]"
          }
        },
        "dffaaf9f49d449ea87c38128c87f4fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0990eee9a1e447bbed771078ddfd602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e83743f167324db3925f09d5adf23847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c26366d099b44c4f9edc041e554e0857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ef1a2b31eb543f890e093156c79be19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10f7fb3d60ba4b649d0f64791dd4d6d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db0c9aa00e3c468eaeb37c3411a5f74d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}