{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Researcher 2: Segmentation Verifier (Week 2)\n",
        "\n",
        "As Researcher 2 in Week 2, my role is to verify the tokenization process and ensure that the segmentation into P (system text), U (user text), and A (assistant's prior text) is correct. I will do this by:\n",
        "1. Loading and validating the tokenizer for the LLaMA-2-7B model.\n",
        "2. Mapping tokens back to text segments for each example.\n",
        "3. Ensuring that the spans for P, U, and A are correctly calculated and that there are no overlaps in these spans.\n",
        "4. Performing a visual check by decoding the tokens back to text and comparing them with the original segments.\n",
        "5. Saving the segmentation metadata to be used later in the pipeline.\n",
        "\n",
        "In addition to the tokenization, I will make sure that the lengths of the spans are consistent and correct. Professor’s note: I should tokenize the whole text (P+U+A) together, rather than tokenizing segments separately, to avoid breaking relationships between segments.\n"
      ],
      "metadata": {
        "id": "6cXeqVMhh4D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install necessary packages and authenticate to HuggingFace\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "from huggingface_hub import login\n",
        "import getpass\n",
        "\n",
        "# Authenticate to HuggingFace (Needed for LLaMA access)\n",
        "print(\"Enter your HuggingFace token (starts with 'hf_'): \")\n",
        "hf_token = getpass.getpass()\n",
        "\n",
        "# Login to HF Hub\n",
        "login(token=hf_token)\n",
        "\n",
        "# Load the tokenizer for LLaMA-2-7b-chat\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "HF_TOKENIZER_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    HF_TOKENIZER_NAME,\n",
        "    use_fast=True\n",
        ")\n",
        "\n",
        "print(\"Loaded tokenizer:\", HF_TOKENIZER_NAME)\n",
        "print(\"Special tokens:\", tokenizer.special_tokens_map)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhQlEv7th8v1",
        "outputId": "d2e4c7c6-080b-4fcd-fee4-a9123806ee26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your HuggingFace token (starts with 'hf_'): \n",
            "··········\n",
            "Loaded tokenizer: meta-llama/Llama-2-7b-chat-hf\n",
            "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization and Segmentation Process\n",
        "\n",
        "In this step, I will:\n",
        "1. **Tokenize the full context**: I will tokenize the entire concatenated text (system prompt P, user query U, and assistant prior text A) together. This will prevent changes in relationships between the segments due to separate tokenization.\n",
        "2. **Calculate spans for P, U, A**: I will derive the start and end positions for each segment (P, U, A) after tokenization.\n",
        "3. **Sanity check**: I will ensure that the sum of lengths of P, U, and A spans equals the total length of the input tokens.\n"
      ],
      "metadata": {
        "id": "iNz5Opd9iQPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load the dataset (selected_all_shuffled.jsonl)\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Define path for dataset\n",
        "selected_path = Path(\"selected_all_shuffled.jsonl\")\n",
        "\n",
        "# Ensure the file exists\n",
        "assert selected_path.exists(), f\"File not found: {selected_path}\"\n",
        "\n",
        "# Load examples from the JSONL file\n",
        "examples = []\n",
        "with selected_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        examples.append(json.loads(line))\n",
        "\n",
        "len(examples), examples[0].keys()  # Print length and keys of the first example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCujsFmyiSLH",
        "outputId": "fcd3dd45-c8f8-4795-ded4-e7ab050539d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,\n",
              " dict_keys(['system_text', 'user_text', 'assistant_prior_text', 'constraint_tags', 'dataset', 'id']))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segment Span Calculation\n",
        "\n",
        "For each example:\n",
        "- **P (system_text)**: Represents the instructions given to the assistant.\n",
        "- **U (user_text)**: Represents the query or the input provided by the user.\n",
        "- **A (assistant_prior_text)**: Represents the assistant’s prior response.\n",
        "\n",
        "I will:\n",
        "1. Tokenize the entire context (P + U + A) together.\n",
        "2. Calculate the token spans for each segment (P, U, A) using the token indices.\n"
      ],
      "metadata": {
        "id": "ntVTU3WXiT6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Segmenting and verifying spans\n",
        "def build_context_and_spans(example, tokenizer):\n",
        "    \"\"\"\n",
        "    Given one example dict with keys:\n",
        "        - id\n",
        "        - system_text (P)\n",
        "        - user_text (U)\n",
        "        - assistant_prior_text (A)\n",
        "\n",
        "    Returns a dict containing:\n",
        "        - id\n",
        "        - input_ids (list of tokenized input)\n",
        "        - p_span (start, end indices for system_text)\n",
        "        - u_span (start, end indices for user_text)\n",
        "        - a_span (start, end indices for assistant_prior_text)\n",
        "    \"\"\"\n",
        "    # Extract texts\n",
        "    P = example.get(\"system_text\", \"\") or \"\"\n",
        "    U = example.get(\"user_text\", \"\") or \"\"\n",
        "    A = example.get(\"assistant_prior_text\", \"\") or \"\"\n",
        "\n",
        "    # Concatenate P, U, A together with fixed separator\n",
        "    sep = \"\\n\\n\"\n",
        "    context_text = P + sep + U + sep + A\n",
        "\n",
        "    # Tokenize full context\n",
        "    full_enc = tokenizer(\n",
        "        context_text,\n",
        "        add_special_tokens=False,\n",
        "        return_attention_mask=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    input_ids = full_enc[\"input_ids\"]\n",
        "    total_len = len(input_ids)\n",
        "\n",
        "    # Calculate the lengths of each segment (P, U, A)\n",
        "    enc_P = tokenizer(\n",
        "        context_text[:len(P)],\n",
        "        add_special_tokens=False,\n",
        "        return_attention_mask=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    len_p = len(enc_P[\"input_ids\"])\n",
        "\n",
        "    # For P + U\n",
        "    prefix_PU_end = len(P) + len(sep) + len(U)\n",
        "    enc_PU = tokenizer(\n",
        "        context_text[:prefix_PU_end],\n",
        "        add_special_tokens=False,\n",
        "        return_attention_mask=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    len_pu = len(enc_PU[\"input_ids\"])\n",
        "    len_u = len_pu - len_p  # U starts after P, so its length is the difference\n",
        "\n",
        "    # The rest is for A\n",
        "    len_a = total_len - len_pu\n",
        "\n",
        "    # Sanity check\n",
        "    assert len_p + len_u + len_a == total_len, (\n",
        "        f\"Length mismatch for id={example.get('id')}: \"\n",
        "        f\"len_p+len_u+len_a={len_p + len_u + len_a}, total_len={total_len}\"\n",
        "    )\n",
        "\n",
        "    # Define the span for each segment (0-based index)\n",
        "    p_start, p_end = 0, len_p\n",
        "    u_start, u_end = p_end, p_end + len_u\n",
        "    a_start, a_end = u_end, u_end + len_a\n",
        "\n",
        "    # Sanity checks for spans\n",
        "    assert 0 <= p_start <= p_end <= total_len\n",
        "    assert 0 <= u_start <= u_end <= total_len\n",
        "    assert 0 <= a_start <= a_end <= total_len\n",
        "    assert a_end == total_len\n",
        "\n",
        "    return {\n",
        "        \"id\": example[\"id\"],\n",
        "        \"dataset\": example.get(\"dataset\", None),\n",
        "        \"input_ids\": input_ids,\n",
        "        \"p_span\": [p_start, p_end],\n",
        "        \"u_span\": [u_start, u_end],\n",
        "        \"a_span\": [a_start, a_end],\n",
        "    }\n",
        "\n",
        "# Run the function for the first 2 examples in the dataset\n",
        "test_meta = [build_context_and_spans(ex, tokenizer) for ex in examples[:2]]\n",
        "test_meta\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPunLIgfiUlm",
        "outputId": "8f72a054-e66a-4678-9ab4-55fe69dd6619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'alpaca:22052',\n",
              "  'dataset': 'alpaca',\n",
              "  'input_ids': [6991,\n",
              "   3034,\n",
              "   675,\n",
              "   278,\n",
              "   1749,\n",
              "   14433,\n",
              "   411,\n",
              "   402,\n",
              "   7982,\n",
              "   1904,\n",
              "   297,\n",
              "   694,\n",
              "   901,\n",
              "   1135,\n",
              "   29871,\n",
              "   29947,\n",
              "   3838,\n",
              "   29889,\n",
              "   13,\n",
              "   13,\n",
              "   13,\n",
              "   13],\n",
              "  'p_span': [0, 18],\n",
              "  'u_span': [18, 20],\n",
              "  'a_span': [20, 22]},\n",
              " {'id': 'alpaca:24364',\n",
              "  'dataset': 'alpaca',\n",
              "  'input_ids': [14350,\n",
              "   263,\n",
              "   26576,\n",
              "   1048,\n",
              "   6709,\n",
              "   29889,\n",
              "   10604,\n",
              "   881,\n",
              "   367,\n",
              "   3109,\n",
              "   1135,\n",
              "   29871,\n",
              "   29947,\n",
              "   29900,\n",
              "   3838,\n",
              "   29889,\n",
              "   13,\n",
              "   13,\n",
              "   13,\n",
              "   13],\n",
              "  'p_span': [0, 16],\n",
              "  'u_span': [16, 18],\n",
              "  'a_span': [18, 20]}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Check for Segmenting\n",
        "\n",
        "I will visualize the output to ensure that:\n",
        "1. Each segment (P, U, A) has been correctly tokenized.\n",
        "2. The spans for P, U, and A are correct and do not overlap.\n",
        "3. The decoded tokens match the original text segments.\n"
      ],
      "metadata": {
        "id": "C4JKDKa_iYdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Visual check for decoded tokens\n",
        "def pretty_visual_check(example, meta, tokenizer, max_chars=300):\n",
        "    \"\"\"\n",
        "    This function prints the original text and the decoded tokenized segments (P, U, A)\n",
        "    to verify that the spans are correctly calculated.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"ID:\", example[\"id\"])\n",
        "    print(\"Dataset:\", example.get(\"dataset\", \"\"))\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    input_ids = meta[\"input_ids\"]\n",
        "    p_start, p_end = meta[\"p_span\"]\n",
        "    u_start, u_end = meta[\"u_span\"]\n",
        "    a_start, a_end = meta[\"a_span\"]\n",
        "\n",
        "    P_dec = tokenizer.decode(input_ids[p_start:p_end], skip_special_tokens=False)\n",
        "    U_dec = tokenizer.decode(input_ids[u_start:u_end], skip_special_tokens=False)\n",
        "    A_dec = tokenizer.decode(input_ids[a_start:a_end], skip_special_tokens=False)\n",
        "\n",
        "    print(\"[system_text] original:\")\n",
        "    print(example.get(\"system_text\", \"\")[:max_chars])\n",
        "    print(\"\\n[P_span decoded]:\")\n",
        "    print(P_dec[:max_chars])\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(\"[user_text] original:\")\n",
        "    print(example.get(\"user_text\", \"\")[:max_chars])\n",
        "    print(\"\\n[U_span decoded]:\")\n",
        "    print(U_dec[:max_chars])\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(\"[assistant_prior_text] original:\")\n",
        "    print(example.get(\"assistant_prior_text\", \"\")[:max_chars])\n",
        "    print(\"\\n[A_span decoded]:\")\n",
        "    print(A_dec[:max_chars])\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# Run visual check for the first 2 examples\n",
        "for ex, meta in zip(examples[:2], test_meta[:2]):\n",
        "    pretty_visual_check(ex, meta, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dObJdryaiZ58",
        "outputId": "92e711f2-41d0-432a-9bf5-bcdcff8bc22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ID: alpaca:22052\n",
            "Dataset: alpaca\n",
            "--------------------------------------------------------------------------------\n",
            "[system_text] original:\n",
            "Summarize the our goals with GPT model in no more than 8 words.\n",
            "\n",
            "[P_span decoded]:\n",
            "Summarize the our goals with GPT model in no more than 8 words.\n",
            "--------------------------------------------------------------------------------\n",
            "[user_text] original:\n",
            "\n",
            "\n",
            "[U_span decoded]:\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[assistant_prior_text] original:\n",
            "\n",
            "\n",
            "[A_span decoded]:\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "================================================================================\n",
            "ID: alpaca:24364\n",
            "Dataset: alpaca\n",
            "--------------------------------------------------------------------------------\n",
            "[system_text] original:\n",
            "Write a poem about spring. Output should be less than 80 words.\n",
            "\n",
            "[P_span decoded]:\n",
            "Write a poem about spring. Output should be less than 80 words.\n",
            "--------------------------------------------------------------------------------\n",
            "[user_text] original:\n",
            "\n",
            "\n",
            "[U_span decoded]:\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[assistant_prior_text] original:\n",
            "\n",
            "\n",
            "[A_span decoded]:\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Segmentation Metadata\n",
        "\n",
        "Once the segmentation is validated, I will save the segmentation metadata in a JSONL file for later use in the pipeline.\n"
      ],
      "metadata": {
        "id": "qgiKjBROidvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save segmentation metadata for all examples\n",
        "segmentation_metadata = []\n",
        "for ex in examples:\n",
        "    meta = build_context_and_spans(ex, tokenizer)\n",
        "    segmentation_metadata.append(meta)\n",
        "\n",
        "# Save the metadata\n",
        "out_path = Path(\"segmentation_metadata.jsonl\")\n",
        "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for meta in segmentation_metadata:\n",
        "        f.write(json.dumps(meta) + \"\\n\")\n",
        "\n",
        "print(f\"Segmentation metadata saved to {out_path}, total examples: {len(segmentation_metadata)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATsm56z0ictB",
        "outputId": "cf4faf29-19f6-4d8a-98e3-8a923229d044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation metadata saved to segmentation_metadata.jsonl, total examples: 300\n"
          ]
        }
      ]
    }
  ]
}